{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-002326ae5ae3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mmake_word_cloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'the'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-002326ae5ae3>\u001b[0m in \u001b[0;36mmake_word_cloud\u001b[0;34m(df, ngram_min, ngram_max, name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_word_cloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmy_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/static/word_cloud/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "data_file = 'app/static/data/reviews.csv'\n",
    "\n",
    "df = pd.read_csv(data_file, encoding='latin1')\n",
    "\n",
    "\n",
    "# This works.\n",
    "def review_by_rating(reviews_df):\n",
    "    reviews_df['rating_desc'] = None\n",
    "    reviews_df['rating_desc'].loc[reviews_df['rating'] < 3] = 'Low Rating (One or Two)'\n",
    "    reviews_df['rating_desc'].loc[reviews_df['rating'] > 3] = 'High Rating (Four or Five)'\n",
    "    reviews_by_rating = reviews_df.groupby('rating_desc')['review'].apply(list)\n",
    "    reviews_by_rating = reviews_by_rating.reset_index(drop=False)\n",
    "    return reviews_by_rating\n",
    "\n",
    "def make_word_cloud(df, ngram_min, ngram_max, name):\n",
    "    ## Change this back to:os.path.abspath(os.path.dirname(__file__)) + '/static/word_cloud/'\n",
    "    my_path = os.path.abspath(os.path.dirname(__file__)) + '/static/word_cloud/'\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    filenames=[]\n",
    "    for ind, row in df.iterrows():\n",
    "        data = row['review']\n",
    "        num_words = 200\n",
    "        ngram_range = (ngram_min, ngram_max)\n",
    "\n",
    "        count_vectorizer = CountVectorizer(lowercase=True,\n",
    "                                           stop_words=stop_words,\n",
    "                                           ngram_range=ngram_range)\n",
    "        counts = count_vectorizer.fit_transform(data)\n",
    "        counts = counts.toarray().sum(axis=0)\n",
    "        count_weighting = dict(zip(count_vectorizer.get_feature_names(), counts))\n",
    "        count_weighting_df = pd.DataFrame.from_dict(count_weighting, orient='index')\n",
    "        count_weighting_df = count_weighting_df.reset_index(drop=False)\n",
    "        count_weighting_df.columns = ['word', 'count']\n",
    "\n",
    "        count_weighting_df = count_weighting_df.sort_values(['count'], ascending=False)\n",
    "        count_weighting_df = count_weighting_df.set_index('word')\n",
    "\n",
    "        word_cloud_freq = count_weighting_df['count'].head(num_words).to_dict()\n",
    "        wordcloud = WordCloud(collocations=False).generate_from_frequencies(word_cloud_freq)\n",
    "        plotname = '{}_{}.png'.format(name,ind+1)\n",
    "        filenames.append(plotname)\n",
    "        url = my_path + plotname\n",
    "        fig = plt.figure(figsize=(10,10))\n",
    "        plt.imshow(wordcloud, cmap=plt.cm.bone, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        fig.savefig(url,transparent = True, bbox_inches = 'tight', pad_inches = 0)\n",
    "    return filenames\n",
    "\n",
    "\n",
    "data_file = 'app/static/data/reviews.csv'\n",
    "\n",
    "\n",
    "make_word_cloud(df, 2,3,'the')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
